{
        "url": "http://localhost:8000/api/v1/topology/eaa43900-cd0f-45ea-81b7-cba778a33298/",
        "id": "eaa43900-cd0f-45ea-81b7-cba778a33298",
        "name": "DC1",
        "organization": "http://localhost:8000/api/v1/organization/c82748ca-94f8-422c-a05e-e9ed0cbe3e81/",
        "nodes": [
            {
                "url": "http://localhost:8000/api/v1/node/6ee430ae-f256-4b15-a6f0-df20f55a6906/",
                "id": "6ee430ae-f256-4b15-a6f0-df20f55a6906",
                "name": "server01",
                "os": "http://localhost:8000/api/v1/image/5f12aa19-831d-44ff-bd9f-0d49d1df44bc/",
                "memory": 500,
                "storage": 500,
                "cpu": 4,
                "interfaces": [
                    {
                        "url": "http://localhost:8000/api/v1/interface/9f93b938-b574-4a03-84f6-2e66481119cd/",
                        "id": "9f93b938-b574-4a03-84f6-2e66481119cd",
                        "name": "eth01",
                        "mac_address": "00:00:00:00:00",
                        "outbound": true,
                        "index": 0
                    },
                    {
                        "url": "http://localhost:8000/api/v1/interface/bd825404-1950-40d9-ab13-f00246d796f2/",
                        "id": "bd825404-1950-40d9-ab13-f00246d796f2",
                        "name": "eth01",
                        "mac_address": "00:00:00:00:00",
                        "outbound": true,
                        "index": 0
                    },
                    {
                        "url": "http://localhost:8000/api/v1/interface/6e2297f6-da40-4bc4-a2f7-2d0b73a77657/",
                        "id": "6e2297f6-da40-4bc4-a2f7-2d0b73a77657",
                        "name": "eth01",
                        "mac_address": "00:00:00:00:00",
                        "outbound": true,
                        "index": 0
                    },
                    {
                        "url": "http://localhost:8000/api/v1/interface/c985f591-6136-49ef-80de-ead72a2b856f/",
                        "id": "c985f591-6136-49ef-80de-ead72a2b856f",
                        "name": "eth01",
                        "mac_address": "00:00:00:00:00",
                        "outbound": true,
                        "index": 0
                    }
                ]
            },
            {
                "url": "http://localhost:8000/api/v1/node/d4843f2f-efc0-4295-9d77-f3430ce2b2c0/",
                "id": "d4843f2f-efc0-4295-9d77-f3430ce2b2c0",
                "name": "server02",
                "os": "http://localhost:8000/api/v1/image/5f12aa19-831d-44ff-bd9f-0d49d1df44bc/",
                "memory": 500,
                "storage": 500,
                "cpu": 4,
                "interfaces": [
                    {
                        "url": "http://localhost:8000/api/v1/interface/3db7f9d1-f2a5-4741-883c-275ed28f935e/",
                        "id": "3db7f9d1-f2a5-4741-883c-275ed28f935e",
                        "name": "eth01",
                        "mac_address": "00:00:00:00:00",
                        "outbound": true,
                        "index": 0
                    },
                    {
                        "url": "http://localhost:8000/api/v1/interface/766f4e1f-5a88-4a82-921a-3f8bb464e7bf/",
                        "id": "766f4e1f-5a88-4a82-921a-3f8bb464e7bf",
                        "name": "eth01",
                        "mac_address": "00:00:00:00:00",
                        "outbound": true,
                        "index": 0
                    },
                    {
                        "url": "http://localhost:8000/api/v1/interface/e043cec7-5600-45f0-97aa-82ef27d03506/",
                        "id": "e043cec7-5600-45f0-97aa-82ef27d03506",
                        "name": "eth01",
                        "mac_address": "00:00:00:00:00",
                        "outbound": true,
                        "index": 0
                    },
                    {
                        "url": "http://localhost:8000/api/v1/interface/53f8ba62-8d59-45e9-a0e1-ef31a494212c/",
                        "id": "53f8ba62-8d59-45e9-a0e1-ef31a494212c",
                        "name": "eth01",
                        "mac_address": "00:00:00:00:00",
                        "outbound": true,
                        "index": 0
                    }
                ]
            },
            {
                "url": "http://localhost:8000/api/v1/node/f0c9f221-b9bf-49ef-8b75-d4c4b0e47a63/",
                "id": "f0c9f221-b9bf-49ef-8b75-d4c4b0e47a63",
                "name": "server03",
                "os": "http://localhost:8000/api/v1/image/5f12aa19-831d-44ff-bd9f-0d49d1df44bc/",
                "memory": 500,
                "storage": 500,
                "cpu": 4,
                "interfaces": []
            }
        ],
        "links": [],
       "documentation": "# Getting started\n\nWelcome to your Cumulus Networks virtual data center. To help you get started, we’ve put together a simple tour of the functionality and common configurations.  The tour starts by using NetQ, a fabric validation system designed to help you understand the state of your network, for an overview of the the devices in the data center and their relationship to each other.\n\nNext, the tour heads to a specific switch, providing a perspective on how it is configured to build the IP fabric.  While there, you’ll explore how a configuration change can be seen in system state and then how rollback works in Cumulus Linux. We will then dive into the NetQ and Kubernetes API integration. Finally, we take a look at the automation system used to deploy the Kubernetes PaaS solution in the data center.\n\n~\n# Step 1: Use NetQ to inspect inventory\n\nClick on the _NetQ_ button to the right to access the Cumulus NetQ user interface in a new browser window. You will be prompted for a username and password `admin` and `CumulusNetQ!` respectively.\n\nThe dashboard shows all NetQ services. Click _Launch Console_ to access the console/cli.  Then, click _Connect_.\n\nType `netq show inventory brief` to see all of the devices in your virtual data center.\n\nYou can also use the NetQ console/cli from your Cumulus in the Cloud web page. Type `netq-shell` in the command pane above to access the NetQ container. From there, you can type `netq show inventory brief` to see the devices in your virtual data center.\n\nTyping `netq show inventory os` provides a baseline view of the operating system installed on all of the devices.\n\n~\n# Step 2: Determine the wiring with NetQ\n\nNow that you have seen the basics of NetQ, you can now explore how your virtual data center is connected. Click on the _Topology_ icon in the toolbar to the left to view the wiring diagram.\n\nNext, you can use NetQ to better understand the exact connectivity. Going back to the NetQ console, type `netq show lldp` to get a full listing of each link. Each row in the output represents a link in your network; for example, the eight links connected to spine02 appear at the end of the listing.\n```\n      <div class=\"tour__test-skills\">\n        <div class=\"tour__test-skills__header\">\n          <h5 class=\"text-white no-margin\">Test your skills</h5>\n        </div>\n        <div class=\"tour__test-skills__body\">\n          <p><strong>Question:</strong> Find the link between spine02 and leaf01; which interfaces connect the two?</p>\n          <p><strong>Answer:</strong> <span class=\"answer__hidden\"><a href=\"#\">Click here to reveal.</a></span><span class=\"answer__revealed\">swp1 on spine02 is connected to swp52 on leaf01.</span></p>\n        </div>\n      </div>\n```\nA full listing of  connectivity can be overwhelming for large deployments. Fortunately, NetQ gives you the ability to focus in on specific hosts. To do so, type `netq leaf* show lldp` to see the wiring for all of the leaf switches. As you can see, this view gives you most of the connectivity information in much shorter form.\n```\n      <div class=\"tour__test-skills\">\n        <div class=\"tour__test-skills__header\">\n          <h5 class=\"text-white no-margin\">Test your skills</h5>\n        </div>\n        <div class=\"tour__test-skills__body\">\n          <p><strong>Question:</strong> What information is missing from this view?</p>\n          <p><strong>Answer:</strong> <span class=\"answer__hidden\"><a href=\"#\">Click here to reveal.</a></span><span class=\"answer__revealed\">The out-of-band network is only partially accounted for.</span></p>\n        </div>\n      </div>\n```\n\n~\n# Step 3: Kubernetes view of NetQ\n\nThis Kubernetes deployment uses a full Layer 3 (IP) fabric, which includes the leaf/spine switches as well as the servers running Routing on the Host.  Docker Engine and Kubernetes are also installed on the servers. This fabric is dual stack IPv4 and IPv6 and has been constructed using BGP, leveraging the BGP unnumbered technology shepherded by technologists at Cumulus Networks.\n\nTo get an overall perspective on the fabric, go to the NetQ console and type `netq show bgp`.  You’ll see a list of devices, their peers, the interface they are peering on, and the Autonomous System Number (ASN) used by BGP to calculate paths through the network.\n\nTo view all the BGP peers from one switch, You’ll notice from the command output as well as the topology diagram that everything is connected to one of the leaf switches, allowing us to simplify the output by typing `netq leaf* show bgp`.\nCheck to ensure all the configured BGP neighbors are up by issuing a `netq check bgp` command.\n\n~\n# Step 4: Switch view of the fabric\n\nNow that we’ve explored the system at a high level, let’s see what the configuration looks like on one of the switches.  Go to the Cumulus in the Cloud console. You need to get to the shell on the oob-mgmt-server, and you can tell you’re there if you see a prompt of `cumulus@oob-mgmt-server:~$`. If your prompt begins with `netq@` (for example `netq@1291b7c84715:/$`), you were using the NetQ console and need to type `exit` return to the base shell.\n\nType `ssh leaf01` to log into leaf01; your prompt should read `cumulus@leaf01:mgmt-vrf:~$`. Type `net show configuration commands` to see leaf01’s configuration. \n```\n      <div class=\"tour__test-skills\">\n        <div class=\"tour__test-skills__header\">\n          <h5 class=\"text-white no-margin\">Test your skills</h5>\n        </div>\n        <div class=\"tour__test-skills__body\">\n          <p>You should now know that net has TAB completion and accepts unique shorthand for all commands, so `net s con c` would have worked as well. Try it out.</p>\n        </div>\n      </div>\n```\n\n~\n# Step 5: Network changes\n\nAfter you have nailed down the basics, you can step things up a notch. Using net, you can now shut  down a link and see how the event propagates through the system.  If you followed the last step, you should be logged into leaf01 (your prompt will read `cumulus@leaf01:mgmt-vrf:~$`). If not, the instructions are repeated below under “getting to leaf01 console.”\n\nYou can access NetQ from everywhere in the system. Type `netq leaf* show bgp` to get an overview of BGP. You’ll notice that leaf01 is connected to spine01 on interface swp51.\n\nUse NCLU (the `net` command) to disable swp51 by typing `net add interface swp51 link down` and then `net commit`.  Then, inspect the outcome with `netq leaf* show bgp`. You’ll notice that the row describing the leaf01 and swp51 is _red_, the state is listed as `Failed`, and the time of last change is very recent (likely `just now`).\n\nTyping `net rollback last` will re-enable the link by rolling back your change, and `netq leaf* show bgp` should give you a clean bill of health. Note, it may take a few seconds for everything to settle.\n\n# Getting to leaf01 console\n\nGo to the Cumulus in the Cloud console, you need to get to the shell on the oob-mgmt-server.  You can tell you’re there if you see a prompt of `cumulus@oob-mgmt-server:~$`. If your prompt begins with `netq@` (for example `netq@1291b7c84715:/$`), you were using the NetQ console and need to type `exit` to get to the base shell.\n\nType `ssh leaf01` to log into leaf01; your prompt should read `cumulus@leaf01:mgmt-vrf:~$`.\n\n~\n# Step 6: Viewing Kubernetes (K8S) deployment with NetQ\n\nNow that you are familiar with the network, let's look at the interaction between the K8S API and NetQ.  We have two K8S deployments, apache1 and apache2 that each have 5 pods distributed across servers 02-04. More information on the K8S deployments and the cluster can be found by accessing the dashboard, via the button to the right of the CITC console.\n      \nFrom any device in the network, view the kubernetes cluster by typing `netq show kubernetes cluster`.  This command will display the master node, the names of the cluster, the status and all the worker nodes.\n\nTwo deployments are running - apache1 and apache2.  Each deployment has 5 POD instances.  View the deployments using `netq show kubernetes deployment name apache*`\n\nCumulus NetQ also provides visibility to show where each of these containers are deployed.  Use the command `netq show kubernetes pod label apache*` to view the location of each POD and the IP address of each POD.\n\nTrace the connectivity path from one POD to another on a different server using the command `netq trace <ip of POD1> from <ip of POD2>`\n\nIn addition to the trace, NetQ allows us to see how each of these pods are connected to the infrastructure.  Use the command `netq show kubernetes deployment name apache* connectivity` to see how each of these deployment pods connect to the leaf switches.\n\n~\n# Step 7: Viewing Kubernetes (K8S) via NetQ\nNetQ also supports direct access into the database, using a SQL-like language.  Since this is an Early Access feature, if you are using the GUI console, turn on this feature by typing the command `netq config add experimental`.  Otherwise, the playbook already turned this feature on for you.\n\nSee which tables are available by issuing the command `netq query show Tables`\n\nWe can also see all the fields available per table.  For example, to see all the fields in the KubePod table, issue the following command\n\n`netq query show fields KubePod`\n\nTo see exactly which running pods are on server02 along with their IP addresses and timestamp, issue the following command:  `netq query 'select node_name, cluster_name, containers, ip_address, owner_name, timestamp, status from KubePod where status=\"Running\" AND node_name=\"server02\"'`\n\nTry some NetQL commands yourself!\n```\n\n      <div class=\"tour__test-skills\">\n        <div class=\"tour__test-skills__header\">\n          <h5 class=\"text-white no-margin\">Test your skills</h5>\n        </div>\n        <div class=\"tour__test-skills__body\">\n          <p><strong>Question:</strong> How do you find which subnet K8S places the Apache pods on server02?</p>\n          <p><strong>Answer:</strong> <span class=\"answer__hidden\"><a href=\"#\">Click here to reveal.</a></span><span class=\"answer__revealed\"> `netq query 'select name, node_name, ip_address from KubePod' | grep server02` </span></p>\n        </div>\n     </div>\n```\n\n~\n# Step 8: Drain pods from a server and view the changes\n\nGet to the shell on the oob-mgmt-server, and you can tell you’re there if you see a prompt of `cumulus@oob-mgmt-server:~$`.  If you were on a different node in the network, type `exit` to return to the oob-mgmt-server.\n\nFrom the oob-mgmt-server, type `ssh server01` to log into server01; your prompt should read `cumulus@server01:~$`.  Server01 is kubernetes master node.\n\nDrain the apache deployments from one server running K8S pods (server02, server03 or server04, identified in Step 6).  We use server03 as an example here. `cumulus@server01:~$ kubectl drain server03 --delete-local-data --force --ignore-daemonsets`\n\nAfter kubernetes drains the server03 node (may take up to 10m), view the changes via NetQ using the command `cumulus@server01:~$ netq show kubernetes pod label apache node server03 changes`.  You will see the pods deleted from server03 database.  This same command can be used from anywhere in the network, try it also from the oob-mgmt-server.\n\nNetQ also allows you to view where the pods were located in the past.  To see what the network was like 10m ago, issue the command `cumulus@oob-mgmt-server:~$ netq show kubernetes pod label apache around 10m`.  You will see which nodes were hosting the deployment 10 minutes ago.\n\nMany more NetQ + Kubernetes features are available, use the TAB; key to explore.\n\n~\n# Step 9: Check the k8s deployment impact of a potential switch outage\n\nNetQ has the ability to display the impact to a orchestrated deployment service if a node were to be removed from the network.  This capability allows a network engineer to gauge when to perform a change to the network.\n\nCheck the impact to the apache service if we were to remove leaf01 from the network by issuing the following command:\n      `cumulus@oob-mgmt-server:~$ netq leaf01 show impact kubernetes deployment name apache1`\n\nThe deployment shown in yellow has partial impact, the pods shown in yellow also have partial impact while the pods shown in green would have no impact. The devices shown in red would have full outage. Try checking a different node!\n\n~\n# Step 10: Get to know the automation\n\nNow that you are familiar with the system, let’s see how it was deployed using Ansible.\n\nLet’s first get to the shell on oob-mgmt-server.  You’ll know you’re there when the prompt reads `cumulus@oob-mgmt-server:~$`. If the prompt reads `cumulus@leaf01:mgmt-vrf:~$`, type exit and you should get our target prompt.\n\nNow, change into the directory that contains the automation by typing `cd /home/cumulus/local-git-repo`.\n\nThe _hosts_ file describes all of the system nodes and allows us to group them for automation; type `less hosts` to see its contents.\n\nThe _roles_ directory tell us which part each device plays in the system.  Type `ls roles` to see these views.\n\nYou’ll notice the _switch-common_ directory that takes care of uniform configuration on all switches; _l3-leaf_ defines our leaf switches and _l3-spine_ defines our spines.\n\nSimilarly, _server-common_ defines configuration that is consistent across all servers with _k8s-master_ and _k8s-worker_ personalizing specific servers for their part in the Kubernetes deployment.\n\nLastly, _setup.yaml_ in the base directory orchestrates the process, it assigns each device to its role in the system and defines the order in which the system is constructed.  Type `less setup.yaml` to look at the blueprint.\n\n~\n# Step 11: Explore on your own\n\nNow that you’ve had a look around, feel free to explore your personal virtual data center.  Use NCLU to change configuration, look at system state and POD deployment and connectivity with NetQ, and modify the automation to see the power of Ansible in network deployments."

    }
