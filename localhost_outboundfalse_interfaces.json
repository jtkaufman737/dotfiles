{
        "url": "http://localhost:8000/api/v1/topology/eaa43900-cd0f-45ea-81b7-cba778a33298/",
        "id": "eaa43900-cd0f-45ea-81b7-cba778a33298",
        "name": "DC1",
        "organization": "http://localhost:8000/api/v1/organization/c82748ca-94f8-422c-a05e-e9ed0cbe3e81/",
        "nodes": [
            {
                "url": "http://localhost:8000/api/v1/node/6ee430ae-f256-4b15-a6f0-df20f55a6906/",
                "id": "6ee430ae-f256-4b15-a6f0-df20f55a6906",
                "name": "server01",
                "os": "http://localhost:8000/api/v1/image/5f12aa19-831d-44ff-bd9f-0d49d1df44bc/",
                "memory": 500,
                "storage": 500,
                "cpu": 4,
                "interfaces": [
                    {
                        "url": "http://localhost:8000/api/v1/interface/9f93b938-b574-4a03-84f6-2e66481119cd/",
                        "id": "9f93b938-b574-4a03-84f6-2e66481119cd",
                        "name": "eth01",
                        "mac_address": "00:00:00:00:00",
                        "outbound": true,
                        "index": 0
                    },
                    {
                        "url": "http://localhost:8000/api/v1/interface/bd825404-1950-40d9-ab13-f00246d796f2/",
                        "id": "bd825404-1950-40d9-ab13-f00246d796f2",
                        "name": "eth01",
                        "mac_address": "00:00:00:00:00",
                        "outbound": true,
                        "index": 0
                    },
                    {
                        "url": "http://localhost:8000/api/v1/interface/6e2297f6-da40-4bc4-a2f7-2d0b73a77657/",
                        "id": "6e2297f6-da40-4bc4-a2f7-2d0b73a77657",
                        "name": "eth01",
                        "mac_address": "00:00:00:00:00",
                        "outbound": true,
                        "index": 0
                    },
                    {
                        "url": "http://localhost:8000/api/v1/interface/c985f591-6136-49ef-80de-ead72a2b856f/",
                        "id": "c985f591-6136-49ef-80de-ead72a2b856f",
                        "name": "eth01",
                        "mac_address": "00:00:00:00:00",
                        "outbound": true,
                        "index": 0
                    }
                ]
            },
            {
                "url": "http://localhost:8000/api/v1/node/d4843f2f-efc0-4295-9d77-f3430ce2b2c0/",
                "id": "d4843f2f-efc0-4295-9d77-f3430ce2b2c0",
                "name": "server02",
                "os": "http://localhost:8000/api/v1/image/5f12aa19-831d-44ff-bd9f-0d49d1df44bc/",
                "memory": 500,
                "storage": 500,
                "cpu": 4,
                "interfaces": [
                    {
                        "url": "http://localhost:8000/api/v1/interface/3db7f9d1-f2a5-4741-883c-275ed28f935e/",
                        "id": "3db7f9d1-f2a5-4741-883c-275ed28f935e",
                        "name": "eth01",
                        "mac_address": "00:00:00:00:00",
                        "outbound": true,
                        "index": 0
                    },
                    {
                        "url": "http://localhost:8000/api/v1/interface/766f4e1f-5a88-4a82-921a-3f8bb464e7bf/",
                        "id": "766f4e1f-5a88-4a82-921a-3f8bb464e7bf",
                        "name": "eth01",
                        "mac_address": "00:00:00:00:00",
                        "outbound": true,
                        "index": 0
                    },
                    {
                        "url": "http://localhost:8000/api/v1/interface/e043cec7-5600-45f0-97aa-82ef27d03506/",
                        "id": "e043cec7-5600-45f0-97aa-82ef27d03506",
                        "name": "eth01",
                        "mac_address": "00:00:00:00:00",
                        "outbound": true,
                        "index": 0
                    },
                    {
                        "url": "http://localhost:8000/api/v1/interface/53f8ba62-8d59-45e9-a0e1-ef31a494212c/",
                        "id": "53f8ba62-8d59-45e9-a0e1-ef31a494212c",
                        "name": "eth01",
                        "mac_address": "00:00:00:00:00",
                        "outbound": true,
                        "index": 0
                    }
                ]
            },
            {
                "url": "http://localhost:8000/api/v1/node/f0c9f221-b9bf-49ef-8b75-d4c4b0e47a63/",
                "id": "f0c9f221-b9bf-49ef-8b75-d4c4b0e47a63",
                "name": "server03",
                "os": "http://localhost:8000/api/v1/image/5f12aa19-831d-44ff-bd9f-0d49d1df44bc/",
                "memory": 500,
                "storage": 500,
                "cpu": 4,
                "interfaces": []
            }
        ],
        "links": [],
        "documentation":"# Getting started \n\n Welcome to your Cumulus Networks virtual data center. To help you get started, we’ve put together a simple tour of the functionality and common configurations. The tour starts by using NetQ, a fabric validation system designed to help you understand the state of your network. NetQ provides you with an overview of the the devices in the data center and their relationship to each other. \n\nNext, the tour heads to a specific switch, providing a perspective on how it is configured to build the IP fabric. While there, you’ll explore how a configuration change can be seen in system state and then how rollback works in Cumulus Linux. Finally, we take a look at the automation system used to deploy the Openstack IaaS solution.\n\nTo get started, connect to the management server in the Cumulus in the Cloud console.  Click on `CHANGE SSH CONNECTION` and select `ssh oob-mgmt-server`, providing username `cumulus` and password `CumulusLinux!`.  Click `CONNECT` and close the popup to access the console of the management server.\n---# Step 1: Use NetQ to inspect inventory\n\nDirect your browser to the link of the 'NetQ' service in the services pane to access the Cumulus NetQ user interface in a new browser window. You will be prompted for a username and password `admin` and `CumulusNetQ!` respectively.\n\nThe dashboard shows all NetQ services. Click _Launch Console_ to access the console/cli.  Then, click _Connect_.\n\nType `netq show inventory brief` to see all of the devices in your virtual data center.\n\nYou can also use the NetQ console/cli from your Cumulus in the Cloud web page. Type `netq-shell` in the command pane above to access the NetQ container. From there, you can type `netq show inventory brief` to see the devices in your virtual data center.\n\nTyping `netq show inventory os` provides a baseline view of the operating system installed on all of the devices.\n\n---\n# Step 2: Determine the wiring with NetQ\n\nNow that you have seen the basics of NetQ, you can now explore how your virtual data center is connected. Click on the _Topology_ icon in the toolbar to the left to view the wiring diagram.\n\nNext, you can use NetQ to better understand the exact connectivity. Going back to the NetQ console, type `netq show lldp` to get a full listing of each link. Each row in the output represents a link in your network; for example, the eight links connected to spine02 appear at the end of the listing.\n\n`Test your skills`\n*Q*: Find the link between spine02 and leaf01; which interfaces connect the two?\n*A*:swp1 on spine02 is connected to swp52 on leaf01\n\nA full listing of  connectivity can be overwhelming for large deployments. Fortunately, NetQ gives you the ability to focus in on specific hosts. To do so, type `netq leaf* show lldp` to see the wiring for all of the leaf switches. As you can see, this view gives you most of the connectivity information in much shorter form.\n\n`Test your skills`\n*Q*: What information is missing from this view?\n*A*: The out-of-band network is only partially accounted for.\n\n---\n# Step 3: NetQ view of BGP\n\nThis OpenStack deployment uses a full Layer 3 (IP) fabric, which includes the leaf/spine switches as well as the servers running OpenStack and Internet access. This fabric is dual stack IPv4 and IPv6 and has been constructed using BGP, leveraging the BGP unnumbered technology shepherded by technologists at Cumulus Networks.\n\nTo get an overall perspective on the fabric, go to the NetQ console and type `netq show bgp`.  You’ll see a list of devices, their peers, the interface they are peering on, and the Autonomous System Number (ASN) used by BGP to calculate paths through the network.\n\nYou’ll notice from the command output as well as the topology diagram that everything is connected to one of the leaf switches, allowing us to simplify the output by typing `netq leaf* show bgp`.\n\n---\n# Step 4: Switch view of the fabric\n\nNow that we’ve explored the system at a high level, let’s see what the configuration looks like on one of the switches.  Go to the Cumulus in the Cloud console. You need to get to the shell on the oob-mgmt-server, and you can tell you’re there if you see a prompt of `cumulus@oob-mgmt-server:~$`. If your prompt begins with `netq@` (for example `netq@1291b7c84715:/$`), you were using the NetQ console and need to type `exit` return to the base shell.\n\nType `ssh leaf01` to log into leaf01; your prompt should read `cumulus@leaf01:mgmt-vrf:~$`. Type `net show configuration commands` to see leaf01’s configuration. \n\n`Test your skills`\n*N*: You should now know that net has TAB completion and accepts unique shorthand for all commands, so `net s con c<` would have worked as well. Try it out.\n\n---\n# Step 5: Network changes\n\nAfter you have nailed down the basics, you can step things up a notch. Using net, shut down a link and see how the event propagates through the system.  If you followed the last step, you should be logged into leaf01 (your prompt will read `cumulus@leaf01:mgmt-vrf:~$`). If not, the instructions are repeated below under *getting to leaf01 console*.\n\nYou can access NetQ from everywhere in the system. Type `netq leaf01 show bgp` to get an overview of BGP. You’ll notice that leaf01 is connected to spine01 on interface swp51.\n\nUse NCLU (the `net` command) to disable swp51 by typing `net add interface swp51 link down` and then `net commit`.  Now, inspect the outcome with `netq leaf01 show bgp`. You’ll notice that the row describing the leaf01 and swp51 is _red_, the state is listed as `Failed`, and the time of last change is very recent (likely `just now`).\n\nTyping `net rollback last` will re-enable the link by rolling back your change, and `netq leaf01 show bgp` should give you a clean bill of health. Note, it may take a few seconds for everything to settle.\n\n# Getting to leaf01 console\n\nGo to the Cumulus in the Cloud console, you need to get to the shell on the oob-mgmt-server.  You can tell you’re there if you see a prompt of `cumulus@oob-mgmt-server:~$`. If your prompt begins with `netq@` (for example `netq@1291b7c84715:/$`), you were using the NetQ console and need to type `exit` to get to the base shell.\n\nType `ssh leaf01` to log into leaf01; your prompt should read `cumulus@leaf01:mgmt-vrf:~$`.\n\n---\n# Step 6: Get to know the automation\n\nNow that you are familiar with the system, let’s see how it was deployed using Ansible.\n\nLet’s first get to the shell on oob-mgmt-server.  You’ll know you’re there when the prompt reads `cumulus@oob-mgmt-server:~$`.  If your prompt begins with `netq@` (for example`netq@1291b7c84715:/$`) or reads `cumulus@leaf01:mgmt-vrf:~$`, type `exit` and you should get our target prompt.\n\nNow, change into the directory that contains the automation by typing `cd /home/cumulus/local-git-repo`.\n\nThe `hosts` file describes all of the system nodes and allows us to group them for automation; type `less hosts` to see its contents.\n\nThe roles directory tell us which part each device plays in the system.  Type `ls roles` to see these views.\n\nYou’ll notice the `switch-common` directory that takes care of uniform configuration on all switches; `l3-leaf` defines our leaf switches and `l3-spine` defines our spines.\n\nSimilarly, `server-common` defines configuration that is consistent across all servers with `openstack-controller/openstack-compute` personalizing specific servers for their part in the OpenStack deployment.\n\nLastly, `setup.yaml` in the base directory orchestrates the process, it assigns each device to its role in the system and defines the order in which the system is constructed.  Type `less setup.yaml` to look at the blueprint.\n\n---\n# Step 7: Explore on your own\n\nNow that you’ve had a look around, feel free to explore your personal virtual data center.  OpenStack Horizon or the `openstack` command line allow you to add virtual machines and tenant networks. You can change network configuration with NCLU, look at system state with NetQ and modify the automation to see the power of Ansible in network deployments.\n\nTo access the Horizon dashboard, use the `default` domain, the `admin` user name, and `CumulusLinux` as the password."
